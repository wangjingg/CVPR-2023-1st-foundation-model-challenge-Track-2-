{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T05:37:12.176120Z",
     "iopub.status.busy": "2023-04-12T05:37:12.175557Z",
     "iopub.status.idle": "2023-04-12T05:37:12.230604Z",
     "shell.execute_reply": "2023-04-12T05:37:12.229407Z",
     "shell.execute_reply.started": "2023-04-12T05:37:12.176085Z"
    },
    "tags": []
   },
   "source": [
    "# Workshop on Foundation Model:1st foundation model challenge\n",
    "\n",
    "# 竞赛简介\n",
    "\n",
    "CVPR作为计算机视觉和模式识别领域的世界级学术顶会，不仅是学者们展示前沿科技成果的学术会议，也是企业界探索前沿应用的一大平台。近年来，随着大模型技术的井喷爆发式发展，基于大模型技术的创新应用正逐步在产业界释放出巨大价值空间。作为人工智能技术领域的领军者与深耕者，百度在大模型技术领域拥有强大的技术优势和深厚技术积累，截至2022年11月，百度自主研发的产业级知识增强大模型体系文心大模型已经包含36个大模型，涵盖基础大模型、任务大模型、行业大模型三级体系，全面满足产业应用需求，构建了业界规模最大的产业大模型体系。作为文心大模型的核心之一，文心·CV大模型VIMER已广泛应用在自动驾驶、云智一体、移动生态等核心业务。\n",
    "\n",
    "为了进一步推动视觉大模型技术的发展，今年百度将在CVPR 2023上举办首届大模型workshop，邀请大模型领域内的顶级学者和精英们共同探讨大模型技术的现状和未来，同时将在智能交通领域举办首个多任务大模型的国际比赛，提供大模型应用技术交流和切磋的平台。我们将于2023年3月28日正式启动第一届大模型技术国际竞赛，向全球开发者开放报名通道。（大赛地址请见文末）\n",
    "\n",
    "本次大模型技术竞赛我们瞄准智能交通方向，开源了Open-TransMind v1.0给选手作为比赛基线，为全球挑战者切磋交流前沿大模型技术提供绝佳机会。\n",
    "\n",
    "关于Open-TransMind v1.0\n",
    "百度在2022年中提出了统一特征表示优化技术（UFO:Unified FeatureOptimization），并发布了当年全球最大视觉模型VIMER-UFO 2.0(文心·CV大模型)，覆盖20+ CV 基础任务，实现了28项公开数据集 SOTA，随后百度Apollo将UFO技术以及智能交通AI能力共同整合为多模态多场景多任务的文心交通大模型之【ERNIE-Traffic-TransMind】，可同时支持点云、视觉、文本三种模态，包含自动驾驶、车路协同、智慧交管、智能网联、智慧停车、智慧高速等多种场景下的百余种交通特性，并且开创式引入了文本图像对话的开放世界理解能力和文本图像模态转化能力，目前已陆续应用到了百度智能交通的各类解决方案和产品线中。\n",
    "\n",
    "\n",
    "一、赛题背景\n",
    "双赛道挑战升级 探索大模型技术革新之道\n",
    "\n",
    "近年来，智慧汽车、人工智能等产业发展，为智能交通发展创造了良好的发展机遇。智能交通相关技术已经渗透到我们的日常生活中，但是现有大模型的多任务处理模式以及传统的感知方法（如分类、检测、分割等）无法满足我们对更广交通场景以及更高自动驾驶水平的追逐。我们从当前实际技术研究中的关键问题出发，设置了两大赛道：\n",
    "\n",
    "赛道一：解决多任务、多数据间冲突的问题\n",
    "之前主流的视觉模型生产流程，通常采用单任务“trainfrom scratch” 方案。每个任务都从零开始训练，各个任务之间也无法相互借鉴。由于单任务数据不足带来偏置问题，实际效果过分依赖任务数据分布，场景泛化效果往往不佳。近两年蓬勃发展的大数据预训练技术，通过使用大量数据学到更多的通用知识，然后迁移到下游任务当中，本质上是不同任务之间相互借鉴了各自学到的知识。基于海量数据获得的预训练模型具有较好的知识完备性，在下游任务中基于少量数据 fine-tuning 依然可以获得较好的效果。不过基于预训练+下游任务 fine-tuning 的模型生产流程，需要针对各个任务分别训练模型，存在较大的研发资源消耗。\n",
    "百度提出的 VIMER-UFO All in One 多任务训练方案，通过使用多个任务的数据训练一个功能强大的通用模型，可被直接应用于处理多个任务。不仅通过跨任务的信息提升了单个任务的效果，并且免去了下游任务 fine-tuning 过程。VIMER-UFO All in One 研发模式可被广泛应用于各类多任务 AI 系统，以智慧城市场景为例，VIMER-UFO 可以用单模型实现人脸识别、人体和车辆ReID等多个任务的 SOTA 效果，同时多任务模型可获得显著优于单任务模型的效果，证明了多任务之间信息借鉴机制的有效性。\n",
    "\n",
    "赛道二：对场景文本图像的理解与感知\n",
    "在交通场景中高性能的图像检索能力对于交通执法、治安治理具有十分重要的作用，传统的图像检索方式通常使用先对图像进行属性识别再通过与期望属性的对比实现检索能力。随着多模态大模型技术的发展，文本与图像的表征统一和模态转换已有广泛应用，使用该能力可以进一步提升图像检索的精度和灵活性。\n",
    "\n",
    "   \n",
    "**获胜队伍可以在[CVPR 2023 foundation model workshop](https://foundation-model.com/)现场宣讲队伍的技术方案也可以提前录制视频。 此外，各Track 前三名会被邀请提交论文（extended abstract论文可以不通过cmt系统提交，regular论文需要通过cmt系统提交) ，论文要求详见[CVPR foundation model workshop论文提交页面](https://foundation-model.com/Paper_Submission)**\n",
    "\n",
    "# Introduction  \n",
    "\n",
    "As a world-class academic summit in the field of computer vision and pattern recognition, CVPR is not only an academic conference for scholars to showcase cutting-edge scientific and technological achievements, but also a major platform for enterprises to explore cutting-edge applications. In recent years, with the explosive development of large model technology, innovative applications based on large model technology are gradually releasing huge value space in the industry. As a leader and deep cultivator in the field of artificial intelligence technology, Baidu has strong technical advantages and profound technological accumulation in the field of large model technology. As of November 2022, Baidu's independently developed industry-level knowledge enhancement large model system, the Wenxin large model, has included 36 large models, covering three levels of systems: basic large model, task large model, and industry large model, fully meeting the needs of industrial applications, The largest industrial model system in the industry has been constructed. As one of the cores of the Wenxin foundation Model, the Wenxin · CV foundation Model VIMER has been widely used in core businesses such as autonomous driving, cloud intelligence integration, and mobile ecology.\n",
    "\n",
    "\n",
    "In order to further promote the development of visual large model technology, Baidu will hold the first large model workshop on CVPR 2023 this year, inviting top scholars and elites in the field of large models to jointly discuss the current situation and future of large model technology. At the same time, it will hold the first international competition for multitasking large models in the field of intelligent transportation, providing a platform for exchange and exchange of large model application technology. On March 28, 2023, we will officially launch the first International Competition for Large Model Technology, opening the registration channel to developers worldwide. (See the end of the article for the address of the competition)\n",
    "\n",
    "\n",
    "In this  model technology competition, we aim at the direction of intelligent transportation and open source Open-TransMind v1.0 as a baseline for competitors, providing a great opportunity for global challengers to exchange cutting-edge foundation model technology.\n",
    "\n",
    "\n",
    "About Open TransMind v1.0\n",
    "\n",
    "In 2022, Baidu proposed the Unified Feature Optimization (UFO) technology, and released the world's largest visual model VIMER-UFO 2.0 (Wenxin · CV foundation Model), covering 20+CV basic tasks, achieving 28 public data sets SOTA. Subsequently, Baidu Apollo combined UFO technology and intelligent transportation AI capabilities into one of the multimodal, multi-scene, and multitasking Wenxin Transportation foundation Models ERNIE-Traffic-TransMind, It can simultaneously support three modes: point cloud, vision, and text, including over a hundred traffic features in multiple scenarios such as autonomous driving, vehicle road collaboration, intelligent traffic management, intelligent Internet connection, intelligent parking, and intelligent highway. It has pioneered the introduction of the open world understanding ability of text image dialogue and text image mode conversion ability, and has been gradually applied to various solutions and product lines of Baidu Intelligent Transportation.\n",
    "\n",
    "\n",
    "1、 Competition background\n",
    "\n",
    "Double Track Challenge Upgrade Explore the Way to Innovate Large Model Technology\n",
    "\n",
    "\n",
    "In recent years, the development of industries such as smart cars and artificial intelligence has created good development opportunities for the development of intelligent transportation. Intelligent transportation related technologies have penetrated into our daily lives, but the multitasking processing mode of existing large models and traditional perceptual methods (such as classification, detection, segmentation, etc.) cannot meet our pursuit of wider traffic scenarios and higher levels of autonomous driving. Starting from the key issues in current practical technical research, we have set up two major tracks:\n",
    "\n",
    "\n",
    "Track 1: Solving conflicts between multiple tasks and data\n",
    "\n",
    "Previously, the mainstream visual model production process typically used a single task \"train from scratch\" scheme. Each task is trained from scratch, and there is no way to learn from each other. Due to the bias problem caused by insufficient single task data, the actual effect relies excessively on the distribution of task data, and the scene generalization effect is often poor. The booming big data pre training technology in the past two years has learned more general knowledge through using a large amount of data, and then migrated to downstream tasks. Essentially, different tasks have borrowed from each other's knowledge. The pre training model based on massive data acquisition has good knowledge completeness, and fine tuning based on small amounts of data can still achieve good results in downstream tasks. However, the model production process based on pre training+downstream task fine tuning requires training models for each task, resulting in significant R&D resource consumption.\n",
    "\n",
    "The VIMER-UFO All in One multitask training scheme proposed by Baidu can be directly applied to processing multiple tasks by training a powerful general model using data from multiple tasks. Not only improves the effectiveness of a single task through cross task information, but also eliminates the downstream task fine-tuning process. The VIMER-UFO All in One R&D model can be widely used in various multitasking AI systems. Taking a smart city scene as an example, VIMER-UFO can use a single model to achieve SOTA effects for multiple tasks such as face recognition, human body, and vehicle ReID. At the same time, the multitask model can achieve significantly better effects than the single task model, demonstrating the effectiveness of the information reference mechanism between multiple tasks.\n",
    "\n",
    "\n",
    "Track 2: Understanding and Perception of Scene Text Images\n",
    "\n",
    "High performance image retrieval capabilities in traffic scenes are very important for traffic law enforcement and public security governance. Traditional image retrieval methods typically use attribute recognition of images before achieving retrieval capabilities by comparing them with desired attributes. With the development of multimodal large model technology, the representation unification and modal transformation of text and images have been widely used. Using this ability can further improve the accuracy and flexibility of image retrieval.\n",
    "\n",
    "**The winner teams are invited to present their solutions on [CVPR 23 foundation model workshop](https://foundation-model.com/)) in person.   Besides, Top 3 teams of each track are invited to submit papers  without using the cmt system（only for extended abstract paper）. Please refer to the [paper submisison page](https://foundation-model.com/Paper_Submission) for more details .**\n",
    "\n",
    "# 赛程赛制\n",
    "参赛选手可以任意选择某个赛道或者同时参加两个赛道，报名赛道2请点击[这里](https://aistudio.baidu.com/aistudio/competition/detail/891/0/introduction)\n",
    "      \n",
    "| 时间（北京时间） | 赛程 | \n",
    "| -------- | -------- |\n",
    "| 2023/3/28 12:00:00 | 正式启动比赛注册报名，开放比赛数据下载 |\n",
    "| 2023/4/1 00:00:00| 开放A榜评测入口 |\n",
    "| 2023/4/8 00:00:00| 发布官方基线 (PaddlePaddle版本)  |\n",
    "| 2023/5/17 23:59:59 | 比赛报名截止，关闭A榜评测入口，锁定比赛A榜排名榜单 |\n",
    "| 2023/5/19 23:59:59 | 公布比赛B榜排名榜单 |\n",
    "| 2023/5/20 00:00:00 -2022/5/22 23:59:59 | B榜榜单TOP10队伍提交代码复查材料（请提前准备，逾期视为放弃） |\n",
    "| 2023/5/23 00:00:00-2022/6/6 23:59:59 | 飞桨比赛组委会审查作品代码及作弊情况，复现成绩 |   \n",
    "| 2023/6/7 12:00:00 | 公布比赛最终排名榜单 |   \n",
    "| 2023/6/12 23:59:59 | 被邀请参会的队伍提交演讲PPT文件，并准备演讲视频（届时会有官方人员联系各队伍） |   \n",
    "| 2023/6/19 | CVPR 2023 foundation model研讨会 |   \n",
    "\n",
    "\n",
    "备注：比赛分A/B榜单，A/B榜单都基于选手提交的同一份提交文件，但是计算分数的节点编号不同。比赛报名截止日期前仅A榜对选手可见，比赛结束后B榜会对选手公布，比赛最终排名按照选手成绩在B榜的排名。\n",
    "\n",
    "# Schedule\n",
    "The competition is divided into two independent tracks, and contestants can choose at will or participate in both tracks at the same time.  Click [ here](https://aistudio.baidu.com/aistudio/competition/detail/891/0/introduction) to join Track 2.\n",
    "| Timeline | Schedule | \n",
    "| -------- | -------- |\n",
    "| 2023/3/28 00:00:00 | Sign up and Release dataset  |\n",
    "| 2023/4/1 00:00:00| Release leaderboard A |\n",
    "| 2023/4/8 00:00:00| Release baseline (PaddlePaddle Version)|\n",
    "| 2023/5/17 EST 23:59:59（GMT+8 5/18 14:59）| Close sign-up and Close  leaderboard A |\n",
    "| 2023/5/19 23:59:59| Release leaderboard B |\n",
    "| 2023/5/20 00:00:00 -2022/5/22 23:59:59 | Submmit code (Only for top 10 teams of Leaderboard B)  |\n",
    "| 2023/5/23 00:00:00-2022/6/6 23:59:59 | Review submmit code and reproduce the results|   \n",
    "| 2023/6/7 12:00:00 |  Release final leaderboard|   \n",
    "| 2023/6/12 23:59:59 | Submmit vedios (Only for top 3 teams)   |   \n",
    "| 2023/6/19 | CVPR 2023 foundation model workshop |   \n",
    "\n",
    "# 比赛奖金  \n",
    "大赛总奖池10,000美元，本赛道总奖池5,000美元，基于飞桨PaddlePaddle的方案才可获得比赛奖金。     \n",
    " \n",
    "| 奖项 | 队伍数 | 奖金（含税） |\n",
    "| -------- | -------- | -------- |\n",
    "| 一等奖  | 1   | 2500美元  |\n",
    "| 二等奖  | 1   | 1500美元 |\n",
    "| 三等奖  | 1   | 1000美元  |\n",
    "\n",
    "**备注：** 最终排名榜单前10名队伍的模型方案，只有使用了飞桨框架并同意开源，才可获得对应奖金，且获奖名次不顺延（若不使用飞桨框架，名次不会被取消，只是无法获得奖金）。\n",
    "# Competition Bonus\n",
    "The total prize pool of this competition is 10000 US dollars, and each track is 5000 US dollars. Only PaddlePaddle solutions can get  bonus. \n",
    "      \n",
    "|  Award | Quantity | Bonus |\n",
    "| -------- | -------- | -------- |\n",
    "| First prize  | 1   | 2500 US dollars  |\n",
    "| Second prize  | 1   | 1500 US dollars |\n",
    "| Third prize  | 1   | 100 US dollars  |\n",
    "      \n",
    "**Note:**  The top 10 teams in the final ranking list can only get the corresponding bonus if they use PaddlePaddle framework and agree to open source, and the award ranking will not be postponed (if they do not use PaddlePaddle framework, the ranking will not be cancelled, but they will not get the bonus).\n",
    "# 参赛对象及要求  \n",
    "**参赛对象：**   \n",
    "本次竞赛面向全社会开放，不限年龄、身份、国籍，相关领域的个人、高等院校、科研机构、企业单位、初创团队等均可报名参赛。百度公司员工可报名参与，但无法获奖。  \n",
    "      \n",
    "**参赛要求：**  \n",
    "支持以个人或团队形式参赛，每个参赛队伍人数最多不超过10人，允许跨单位自由组队，但每人只能参加一支队伍。  \n",
    "   \n",
    " # Participants and requirements\n",
    "**Participants:**  \n",
    "The competition is open to the whole society, regardless of age, identity and nationality. Individuals in related fields, universities, scientific research institutions, enterprises and start-up teams can sign up for the competition.  Baidu employees can sign up to participate, but cannot win the prize.  \n",
    "  \n",
    "**Entry requirements:**  \n",
    "Individual or team participation is supported. The maximum number of participants in each team is no more than 15. Cross units are allowed to form teams freely, but each team can only participate in one team.  \n",
    "\n",
    "  # 参赛方式及规则  \n",
    "（1）\t所有参赛选手都必须在**百度大脑AI Studio平台**注册报名；      \n",
    "（2）\t参赛选手需确保注册时提交信息准确有效，所有的比赛资格及奖金支付均以提交信息为准；      \n",
    "（3）\t参赛选手报名后可在“我的团队”页面组队。每支队伍需指定一名队长，队伍成员总数**最多不超过10人**,每名参赛选手只能参加一支队伍，一旦发现某选手以注册多个账号的方式参加多支队伍，将取消相关队伍的参赛资格；        \n",
    "（4）队伍名的设定不得违反中国法律法规或社会公序良俗，且**参赛队伍命名中不可出现“百度官方”，“飞桨官方”，“paddle官方”，“官方baseline”等字样。若命名违规的队伍在收到比赛主办方警告后仍未修改队伍名称，比赛主办方有权解散该队伍** ；     \n",
    "（5）\t除主办方提供的数据集外，参赛选手不得使用任何其他渠道的标注数据；  \n",
    "（6）\t参赛队伍可在参赛期间随时上传测试集的预测结果，每天最多评测5次，比赛管理系统会实时更新各队伍的当前最高成绩与当前最新榜单排名情况；    \n",
    "（7）\t选手可以使用的开源预训练模型或者自己训练的预训练模型，但非开源预训练模型的生产过程必须可复现；  \n",
    "（8）\t赛事交流微信群：     \n",
    " <img src=\"https://bj.bcebos.com/v1/ai-studio-match/file/48b873cb6af1429eb4f52c2a368b915a78d0f39dae7c4a0d8e4efe626f4322e9?authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2023-03-27T13%3A29%3A09Z%2F-1%2F%2F78cc6767a620546b9d3e72c60b2e0842228cbb1a113be84c3238ff48c08a8504\" alt=\"图\" width=\"200\" height=\"200\">  \n",
    " \n",
    " # Competition  rules\n",
    "(1) All contestants must register in **AI studio platform** ; \n",
    "(2) Contestants should ensure that the information submitted during registration is accurate and valid, and all qualifications and bonus payments are subject to the information submitted;  \n",
    "(3) Contestants can form teams on the \"my team\" page after they sign up. Each team needs to appoint a team leader, and the total number of team members **shall not exceed 10**. Each contestant can only participate in one team. Once a contestant is found to participate in multiple teams by registering multiple accounts, the qualification of relevant teams will be cancelled;  \n",
    "(4) The name of the team shall not be set in violation of Chinese laws and regulations or public order and good customs, and the words \"Baidu official\", \"feioar official\", \"paddle official\", \"official baseline\" and so on shall not appear in the name of participating teams. If the name of the team is not changed after receiving the warning from the organizer, the organizer has the right to dissolve the team;  \n",
    "(5) Except for the data set provided by the organizer, contestants are not allowed to use the marked data from any other channels;  \n",
    "(6) The teams can upload the forecast results of the test set at any time during the competition. In the stage a, the team can be evaluated 5 times a day at most. The competition management system will update the current highest score and the latest ranking of each team in real time;  \n",
    "(7) Competitors can use open-source pre-trained models or pre-trained models trained by themselves, but the production process of non-open-source pre-trained models must be reproducible.\n",
    "(8) WeChat group:      \n",
    " <img src=\"https://bj.bcebos.com/v1/ai-studio-match/file/48b873cb6af1429eb4f52c2a368b915a78d0f39dae7c4a0d8e4efe626f4322e9?authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2023-03-27T13%3A29%3A09Z%2F-1%2F%2F78cc6767a620546b9d3e72c60b2e0842228cbb1a113be84c3238ff48c08a8504\" alt=\"图\" width=\"200\" height=\"200\">  \n",
    "\n",
    " \n",
    " # 反作弊说明  \n",
    "（1）\t参与者禁止注册多账户报名，经发现将取消成绩并严肃处理。  \n",
    "（2）\t参与者禁止在考核技术能力的范围外利用规则漏洞或技术漏洞等不良途径提高成绩排名，经发现将取消成绩并严肃处理。   \n",
    "（3）\tAI Studio将收集选手信息以及代码、模型、系统报告用于成绩评定、比赛通知等相关比赛事项。  \n",
    " \n",
    "# Anti cheating instructions\n",
    "\n",
    "(1) Participants are not allowed to register for multiple accounts. Once found, their scores will be cancelled and dealt with seriously.  \n",
    "(2) Participants are not allowed to use rules loopholes or technical loopholes and other bad ways to improve their performance ranking outside the scope of technical ability assessment. Once found, their performance will be cancelled and dealt with seriously.  \n",
    "(3) AI Studio will collect player information, code, model and system report for performance evaluation, competition notice and other related competition matters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 赛道二：Cross-Modal Image Retrieval Track\n",
    "## 赛题背景\n",
    "交通场景中高性能的图像检索能力对于交通执法、治安治理具有十分重要的作用，传统的图像检索方式通常使用先对图像进行属性识别再通过与期望属性的对比实现检索能力。随着多模态大模型技术的发展，文本与图像的表征统一和模态转换已有广泛应用，使用该能力可以进一步提升图像检索的精度和灵活性。\n",
    "## 赛题任务\n",
    "本赛道旨在提升交通场景中文本图像检索的精度。因此我们将多种公开数据集以及网络数据中的交通参与者图像进行了文本描述标注从而构建了多对多的图像-文本对，选手可以在此基础上进行多模态技术的研究工作，提升文本检索图像的精度。\n",
    "### 数据集介绍\n",
    "本赛题构建了一个多交通参与者的文本检索图像数据集，该数据集以开源数据集为基础，同时使用网络爬虫技术扩充数据的丰富度。在标注方面，首先利用CV大模型丰富图像标注属性，然后利用大语言模型构造图像对应的文本标注。目前数据集的总量有153728张，其中训练集136117张，评测集17611张。数据集包含行人和车辆2类交通参与者，数据分布具体见下表。\n",
    "|  类别   | 训练集  | 测试集 |\n",
    "|  ----  | ----  | ---- |\n",
    "| 行人  | 90000 | 10000 |\n",
    "| 车辆  | 46117 | 7611 |\n",
    "| 总数  | 136117 | 17611 |\n",
    "### 数据说明\n",
    "标注格式示例：\n",
    "```\n",
    "图片$属性标注$文本\n",
    "pmitevanhx.jpg$A white Audi.$This is a white Audi.\n",
    "090000$0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0$A female pedestrian is someone who walks on foot, is between 18 and 60 years old, with her body facing the camera. She is in a short sleeve shirt with an upper logo.\n",
    "```\n",
    "由于存在一个文本对应多张图像 or 一张图像对应多个文本的情况，在评测过程中使用「属性文本」对这种情况进行处理。\n",
    "## 评价指标\n",
    "本赛题聚焦在文本检索图像的精度，因此使用的评价指标为平均检索精度$mAP@K$, $mAP$全称mean Average Precision，$K$代表的是前$K$个检索结果参与评价，本赛题取$K=10$，平均检索精度$mAP@K$的计算如下式所示：\n",
    "$$mAP@K=\\frac{1}{m} * \\sum_{i=1}^{K}{p(i)*\\Delta r(i)}，$$\n",
    "其中，*m*为评测集中文本的总数，$p(i)$指的是$topi$检索结果的precision, $\\Delta r(i)$的计算如下式所示：\n",
    "$$\\Delta r(i)=r(i)-r(i-1)，$$\n",
    "其中，$r(i)$为$topi$结果的recall, $r(0)=0$。\n",
    "\n",
    "## 提交格式\n",
    "文件格式：JSON\n",
    "\n",
    "内容格式：\n",
    "```\n",
    "{\n",
    "    {\n",
    "        'results':\n",
    "        [\n",
    "            {\n",
    "                'text': text_1,\n",
    "                'image_names': [image_name_1, image_name_2, ..., image_name_10]\n",
    "            },\n",
    "            {\n",
    "                'text': text_2,\n",
    "                'image_names': [image_name_1, image_name_2, ..., image_name_10]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "**text_1, text2, ... 的顺序需要和test/test_text.txt中的顺序保持一致。**\n",
    "\n",
    "参考示例：\n",
    "```\n",
    "{\n",
    "    {\n",
    "        'results':\n",
    "        [\n",
    "            {\n",
    "                'text': 'This is a grey Chery Sedan.',\n",
    "                'image_names': ['vehicle_0001886.jpg', 'vehicle_0000196.jpg', 'vehicle_0002886.jpg', 'vehicle_0007116.jpg', 'vehicle_0001256.jpg', 'vehicle_0007852.jpg', 'vehicle_0003548.jpg', 'vehicle_0008215.jpg', 'vehicle_0000851.jpg', 'vehicle_0007531.jpg']\n",
    "            },\n",
    "            {\n",
    "                'text': 'This is a black Volkswagen Sedan.',\n",
    "                'image_names': ['vehicle_0003234.jpg', 'vehicle_0009561.jpg', 'vehicle_0008521.jpg', 'vehicle_0006540.jpg', 'vehicle_0000851.jpg', 'vehicle_0003612.jpg', 'vehicle_0008124.jpg', 'vehicle_0000513.jpg', 'vehicle_0004811.jpg', 'vehicle_0001577.jpg']\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 比赛说明\n",
    "该比赛分A/B榜单，A榜单基于选手提交预测结果进行打分和排序，B榜单针对topk选手提交的code进行复现，复现失败则排名无效。\n",
    "比赛提交截止日期前仅A榜对选手可见，比赛结束后B榜会对选手公布，比赛最终排名按照选手成绩在A榜和B榜联和的排名。\n",
    "注意：请确保提交至B榜上的代码脚本能够顺利运行。\n",
    "\n",
    "# Track 2: Cross-Modal Image Retrieval Track\n",
    "\n",
    "## Background\n",
    "High-performance image retrieval in traffic scenes plays a crucial role in traffic law enforcement and public security management. Traditional image retrieval methods usually use attribute recognition to retrieve images by comparing with the expected attributes. With the development of multi-modal large model technology, the unification of text and image representation and modal conversion has been widely used. Using this ability can further improve the accuracy and flexibility of image retrieval.\n",
    "\n",
    "## Task\n",
    "The goal of this track is to improve the accuracy of text-based image retrieval in traffic scenes. Therefore, we have annotated text descriptions for images of traffic participants from various public datasets and online sources to construct many-to-many image-text pairs. Participants can conduct research on multimodal techniques based on these pairs to improve the accuracy of text retrieval for images.\n",
    "\n",
    "### Dataset Introduction\n",
    "This competition constructed a text retrieval image dataset with multiple traffic participants, based on open-source datasets and using web crawler technology to enrich the data. In terms of annotation, first, a large CV model is used to enrich the image annotation attributes, and then a large language model is used to construct the corresponding text annotation for the image. Currently, the dataset has a total of 153,728 images, including 136,117 images in the training set and 17,611 images in the evaluation set. The dataset contains two categories of traffic participants: pedestrians and vehicles, and the specific data distribution is shown in the table below.\n",
    "|  Category   | Training Set | Test Set\t |\n",
    "|  ----  | ----  | ---- |\n",
    "| 行人  | 90000 | 10000 |\n",
    "| 车辆  | 46117 | 7611 |\n",
    "| 总数  | 136117 | 17611 |\n",
    "\n",
    "### Data Description\n",
    "Annotation format example:\n",
    "```\n",
    "Image$Attribute Annotation$Text\n",
    "pmitevanhx.jpg$A white Audi.$This is a white Audi.\n",
    "090000$0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0$A female pedestrian is someone who walks on foot, is between 18 and 60 years old, with her body facing the camera. She is in a short sleeve shirt with an upper logo.\n",
    "```\n",
    "Due to the existence of cases where one text corresponds to multiple images or one image corresponds to multiple texts, during the evaluation process, the \"attribute text\" is used to handle such cases.\n",
    "\n",
    "## Evaluation Metric\n",
    "This competition focuses on the accuracy of text retrieval for images, so the evaluation metric used is the mean Average Precision ($mAP@K$). Here, $K$ represents the number of retrieval results ($top K$) used in the evaluation, and this competition takes $K=10$. The calculation of $mAP@K$ is as follows:\n",
    "$$mAP@K=\\frac{1}{m} * \\sum_{i=1}^{K}{p(i)*\\Delta r(i)}，$$\n",
    "where $m$ is the total number of texts in the evaluation set, $p(i)$ is the precision of the $topi$ retrieval results, and $\\Delta r(i)$ is calculated as follows:\n",
    "$$\\Delta r(i)=r(i)-r(i-1)，$$\n",
    "where $r(i)$ is the recall of the topi result, and $r(0)=0$.\n",
    "\n",
    "## Submission Format\n",
    "File format: JSON\n",
    "\n",
    "Content format:\n",
    "```\n",
    "{\n",
    "    {\n",
    "        'results':\n",
    "        [\n",
    "            {\n",
    "                'text': text_1,\n",
    "                'image_names': [image_name_1, image_name_2, ..., image_name_10]\n",
    "            },\n",
    "            {\n",
    "                'text': text_2,\n",
    "                'image_names': [image_name_1, image_name_2, ..., image_name_10]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**The order of text_1, text2, ... must be consistent with the order in test/test_text.txt.**\n",
    "\n",
    "Example:\n",
    "```\n",
    "{\n",
    "    {\n",
    "        'results':\n",
    "        [\n",
    "            {\n",
    "                'text': 'This is a grey Chery Sedan.',\n",
    "                'image_names': ['vehicle_0001886.jpg', 'vehicle_0000196.jpg', 'vehicle_0002886.jpg', 'vehicle_0007116.jpg', 'vehicle_0001256.jpg', 'vehicle_0007852.jpg', 'vehicle_0003548.jpg', 'vehicle_0008215.jpg', 'vehicle_0000851.jpg', 'vehicle_0007531.jpg']\n",
    "            },\n",
    "            {\n",
    "                'text': 'This is a black Volkswagen Sedan.',\n",
    "                'image_names': ['vehicle_0003234.jpg', 'vehicle_0009561.jpg', 'vehicle_0008521.jpg', 'vehicle_0006540.jpg', 'vehicle_0000851.jpg', 'vehicle_0003612.jpg', 'vehicle_0008124.jpg', 'vehicle_0000513.jpg', 'vehicle_0004811.jpg', 'vehicle_0001577.jpg']\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Competition Details\n",
    "This competition is divided into A/B lists. The A list is based on the prediction results submitted by the participants for scoring and ranking, while the B list is for reproducing the code submitted by the top K participants. If the reproduction fails, the ranking will be invalid.\n",
    "\n",
    "Before the competition submission deadline, only the A list is visible to the participants. After the competition ends, the B list will be made public to the participants. The final ranking of the competition is based on the participants' scores in the A and B lists combined.\n",
    "\n",
    "Note: Please make sure that the code script submitted to the B list can run smoothly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track2 Demo\n",
    "\n",
    "本demo基于4卡V100环境，且训练前需要申请比赛白名单，其他情况请参考github：https://github.com/xiteng01/CVPR2023_foundation_model_Track2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-18T07:38:03.154162Z",
     "iopub.status.busy": "2023-05-18T07:38:03.153517Z",
     "iopub.status.idle": "2023-05-18T07:38:43.363301Z",
     "shell.execute_reply": "2023-05-18T07:38:43.362211Z",
     "shell.execute_reply.started": "2023-05-18T07:38:03.154128Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data203278\r\n",
      "model_0005299.pdmodel\t\t    train.tar\r\n",
      "paddlepaddle_gpu-2.3.2.post112.whl  val.tar\r\n",
      "test.tar\t\t\t    vitbase_clip.pdparams\r\n",
      "track2_submit_example.json\r\n",
      "/home/aistudio/data/data203278\r\n",
      "model_0005299.pdmodel\t\t    train\r\n",
      "paddlepaddle_gpu-2.3.2.post112.whl  train.tar\r\n",
      "test\t\t\t\t    val\r\n",
      "test.tar\t\t\t    val.tar\r\n",
      "track2_submit_example.json\t    vitbase_clip.pdparams\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data\n",
    "!ls /home/aistudio/data/data203278\n",
    "# 解压数据\n",
    "!cd /home/aistudio/data/data203278 && pwd && tar xf train.tar && tar xf val.tar && tar xf test.tar\n",
    "!ls /home/aistudio/data/data203278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-04-12T11:44:22.624452Z",
     "iopub.status.busy": "2023-04-12T11:44:22.624203Z",
     "iopub.status.idle": "2023-04-12T11:44:23.427592Z",
     "shell.execute_reply": "2023-04-12T11:44:23.426567Z",
     "shell.execute_reply.started": "2023-04-12T11:44:22.624427Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_map_json.py  logs\r\n",
      "configs\t\t modeling\r\n",
      "data\t\t outputs\r\n",
      "datasets\t paddlepaddle_gpu-2.3.2.post112-cp37-cp37m-linux_x86_64.whl\r\n",
      "detectron2\t pretrained\r\n",
      "engine\t\t README_ch.md\r\n",
      "evaluation\t README.md\r\n",
      "fastreid\t requirements.txt\r\n",
      "infer_json.json  scripts\r\n",
      "infer_json.py\t solver\r\n",
      "infer.txt\t test.py\r\n",
      "layers\t\t tools\r\n",
      "LICENSE\t\t utils\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "#!ls /home/aistudio/work\n",
    "!tar xf CVPR2023_foundation_model_Track2-main.tar\n",
    "!ls /home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-04-12T11:44:23.429170Z",
     "iopub.status.busy": "2023-04-12T11:44:23.428903Z",
     "iopub.status.idle": "2023-04-12T11:45:59.768836Z",
     "shell.execute_reply": "2023-04-12T11:45:59.767284Z",
     "shell.execute_reply.started": "2023-04-12T11:44:23.429145Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll\r\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Processing ./paddlepaddle_gpu-2.3.2.post112-cp37-cp37m-linux_x86_64.whl\r\n",
      "Collecting fvcore==0.1.5.post20220119\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/35/bc/84d11b380ce08e384f8794240432dc3d95eb17237073f0cb1c564cd91084/fvcore-0.1.5.post20220119.tar.gz (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m166.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting torchvision==0.8.2\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting cloudpickle==2.0.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/07/3c/bf72ebd3e78eb1ef773f4f0650ecdc29c6454aeafe9c08f6da3f227dd2bc/cloudpickle-2.0.0-py3-none-any.whl (25 kB)\r\n",
      "Collecting omegaconf==2.1.1\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/66/c8/7ef11e12f3844b210add2e003abf8a0c7981ce7b5553dc630b635e7b905e/omegaconf-2.1.1-py3-none-any.whl (74 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m894.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting pycocotools==2.0.4\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/75/5c/ac61ea715d7a89ecc31c090753bde28810238225ca8b71778dfe3e6a68bc/pycocotools-2.0.4.tar.gz (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting timm==0.4.12\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/90/fc/606bc5cf46acac3aa9bd179b3954433c026aaf88ea98d6b19f5d14c336da/timm-0.4.12-py3-none-any.whl (376 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting typing_extensions==4.5.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting paddleseg==2.7.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c1/30/c13ae53b180443edc4c88921b6a3f7999e7a3b831ca3b6990e966667148d/paddleseg-2.7.0-py3-none-any.whl (349 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.4/349.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting paddledet==2.6.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5b/b3/9d4335ef2a6bc53665bd262fe0288399ae4c9fc2cac3f6604bc50573ae61/paddledet-2.6.0-py3-none-any.whl (812 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting opencv-python==4.4.0.46\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1b/2d/62eba161d3d713e1720504de1c25d439b02c85159804d9ecead10be5d87e/opencv_python-4.4.0.46-cp37-cp37m-manylinux2014_x86_64.whl (49.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting hydra-core==1.1.1\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/24/27/c8f0ccc24512fadfa53d30ea26a588c04de962e9670b3c28fe51595a9b7a/hydra_core-1.1.1-py3-none-any.whl (145 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting sacred==0.8.2\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f4/8c/b99f668e8ca9747dcd374bb46cac808e58f3cb8e446df1b3e667f6be9778/sacred-0.8.2-py2.py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard==2.5.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting ftfy\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e1/1e/bf736f9576a8979752b826b75cbd83663ff86634ea3055a766e2d8ad3ee5/ftfy-6.1.1-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m999.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting regex\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/42/d8/8a7131e7d0bf237f7bcd3191541a4bf21863c253fe6bee0796900a1a9a29/regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fvcore==0.1.5.post20220119->-r requirements.txt (line 2)) (1.19.5)\r\n",
      "Collecting yacs>=0.1.6\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fvcore==0.1.5.post20220119->-r requirements.txt (line 2)) (5.1.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fvcore==0.1.5.post20220119->-r requirements.txt (line 2)) (4.27.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fvcore==0.1.5.post20220119->-r requirements.txt (line 2)) (1.1.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fvcore==0.1.5.post20220119->-r requirements.txt (line 2)) (8.2.0)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fvcore==0.1.5.post20220119->-r requirements.txt (line 2)) (0.8.3)\r\n",
      "Collecting iopath>=0.1.7\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/72/73/b3d451dfc523756cf177d3ebb0af76dc7751b341c60e2a21871be400ae29/iopath-0.1.10.tar.gz (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m976.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting torch==1.7.1\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\r\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.8\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycocotools==2.0.4->-r requirements.txt (line 6)) (2.2.3)\r\n",
      "Requirement already satisfied: visualdl>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleseg==2.7.0->-r requirements.txt (line 9)) (2.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleseg==2.7.0->-r requirements.txt (line 9)) (3.0.12)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleseg==2.7.0->-r requirements.txt (line 9)) (1.6.3)\r\n",
      "Requirement already satisfied: prettytable in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleseg==2.7.0->-r requirements.txt (line 9)) (0.7.2)\r\n",
      "Requirement already satisfied: sklearn==0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddleseg==2.7.0->-r requirements.txt (line 9)) (0.0)\r\n",
      "Requirement already satisfied: Cython in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddledet==2.6.0->-r requirements.txt (line 10)) (0.29)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddledet==2.6.0->-r requirements.txt (line 10)) (56.2.0)\r\n",
      "Collecting lap\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bf/64/d9fb6a75b15e783952b2fec6970f033462e67db32dc43dfbb404c14e91c2/lap-0.4.0.tar.gz (1.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting typeguard\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/62/7d206b0ac6fcbb163215ecc622a54eb747f85ad86d14bc513a834442d0f6/typeguard-3.0.2-py3-none-any.whl (30 kB)\r\n",
      "Collecting shapely\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1d/a4/931d0780f31f3ea8c4f9ef6464a2825137c5241e6707a5fb03bef760a7eb/shapely-2.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting terminaltables\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c4/fb/ea621e0a19733e01fe4005d46087d383693c0f4a8f824b47d8d4122c87e0/terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting motmetrics\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2f/d9/7b77e1e2db80b6f8133065ffbccdaa3c911df5f95a7af30829fcaa10a3d7/motmetrics-1.4.0-py3-none-any.whl (161 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyclipper\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/21/b9/f8bd7bb8b04906ac2f93518ae22040c99db9dfc9faf2a29d444c6469b6a3/pyclipper-1.3.0.post4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (604 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.2/604.2 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from hydra-core==1.1.1->-r requirements.txt (line 12)) (5.9.0)\r\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacred==0.8.2->-r requirements.txt (line 13)) (0.4.4)\r\n",
      "Collecting jsonpickle<2.0,>=1.2\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e9/ec/35910cf6ab87f8a013036f01f732f871a23b6058123a7bd0c7b08fbbc937/jsonpickle-1.5.2-py2.py3-none-any.whl (37 kB)\r\n",
      "Requirement already satisfied: GitPython in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacred==0.8.2->-r requirements.txt (line 13)) (3.1.14)\r\n",
      "Collecting docopt<1.0,>=0.3\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz (25 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting py-cpuinfo>=4.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e0/a9/023730ba63db1e494a271cb018dcd361bd2c917ba7004c3e49d5daf795a2/py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting munch<3.0,>=2.0.2\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl (10 kB)\r\n",
      "Requirement already satisfied: packaging>=18.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacred==0.8.2->-r requirements.txt (line 13)) (21.3)\r\n",
      "Requirement already satisfied: wrapt<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacred==0.8.2->-r requirements.txt (line 13)) (1.12.1)\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (1.10.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (1.35.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (2.24.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (0.8.1)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (3.20.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (0.36.2)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (0.16.0)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (0.4.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 14)) (3.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn==0.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (0.24.2)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2.post112->-r requirements.txt (line 1)) (4.4.2)\r\n",
      "Requirement already satisfied: paddle-bfloat==0.1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2.post112->-r requirements.txt (line 1)) (0.1.7)\r\n",
      "Requirement already satisfied: astor in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2.post112->-r requirements.txt (line 1)) (0.8.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2.post112->-r requirements.txt (line 1)) (1.16.0)\r\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2.post112->-r requirements.txt (line 1)) (3.3.0)\r\n",
      "Collecting wcwidth>=0.2.5\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/f4/c0584a25144ce20bfcf1aecd041768b8c762c1eb0aa77502a3f0baa83f11/wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 14)) (4.0.0)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 14)) (4.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 14)) (0.2.7)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.5.0->-r requirements.txt (line 14)) (1.3.0)\r\n",
      "Collecting portalocker\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8c/df/d4f711d168524f5aebd7fb30969eaa31e3048cf8979688cde3b08f6e5eb8/portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from jsonpickle<2.0,>=1.2->sacred==0.8.2->-r requirements.txt (line 13)) (4.2.0)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r requirements.txt (line 6)) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r requirements.txt (line 6)) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r requirements.txt (line 6)) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r requirements.txt (line 6)) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r requirements.txt (line 6)) (3.0.9)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.5.0->-r requirements.txt (line 14)) (2.8)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.5.0->-r requirements.txt (line 14)) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.5.0->-r requirements.txt (line 14)) (1.25.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard==2.5.0->-r requirements.txt (line 14)) (2019.9.11)\r\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (1.1.1)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (1.0.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (1.1.5)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (0.8.53)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from GitPython->sacred==0.8.2->-r requirements.txt (line 13)) (4.0.5)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-resources->hydra-core==1.1.1->-r requirements.txt (line 12)) (3.8.1)\r\n",
      "Collecting xmltodict>=0.12.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/db/fd0326e331726f07ff7f40675cd86aa804bfd2e5016c727fa761c934990e/xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (3.0.0)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (7.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (1.1.0)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (2.8.0)\r\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython->sacred==0.8.2->-r requirements.txt (line 13)) (3.0.5)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 14)) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.5.0->-r requirements.txt (line 14)) (3.1.0)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (3.9.9)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (0.18.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->sklearn==0.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->sklearn==0.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (2.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl>=2.2.0->paddleseg==2.7.0->-r requirements.txt (line 9)) (2.0.1)\r\n",
      "paddlepaddle-gpu is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Building wheels for collected packages: fvcore, pycocotools, antlr4-python3-runtime, docopt, iopath, lap\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220119-py3-none-any.whl size=65259 sha256=1d5007347743c8daff100f4fcb4a23958e7a006bb5235f83a9aeb557e021b437\r\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/3c/80/b9/05c9b6f5219f8f1655911e57afc406163207e1f361ffd112fc\r\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp37-cp37m-linux_x86_64.whl size=273779 sha256=bc86b0f122a1fc6acdc1296a48e96d9d6bc3b14eac1fd0f953a7f049dfe70b1c\r\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/c0/01/5f/670dfd20204fc9cc6bf843db4e014acb998f411922e3abc49f\r\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=698ab51561d2c56b5ffe44cee512493460dc89ccbb57843100057aae6923458b\r\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/90/f3/a7/aab9ff1bf131cd593812fa6e63126845f10a46037fb7913b22\r\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=d3a9966859957b96dc69342d7021a450d6d51dfd53673b2da7234f07c86688bb\r\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/17/d9/6e/6b3eb21ad36a388d8d7eb1c1c61e34c627876458e6fc37790d\r\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31543 sha256=638df2634638cf975b205b71b463b5cc10533a5d74f68de9fa5e95107598fd2b\r\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/7b/48/4c/9a0693308f0176cfc02126cc5ce3c00df2ffa4bf927d606906\r\n",
      "  Building wheel for lap (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for lap: filename=lap-0.4.0-cp37-cp37m-linux_x86_64.whl size=1593870 sha256=ab04a8f8bf272ce6f60ff9e235decab771a3cc8208105e5df30c9c22bd203896\r\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/5c/d0/d2/e331d17a999666b1e2eb99743cfa1742629f9d26c55c657001\r\n",
      "Successfully built fvcore pycocotools antlr4-python3-runtime docopt iopath lap\r\n",
      "Installing collected packages: wcwidth, tensorboard-plugin-wit, pyclipper, py-cpuinfo, lap, docopt, antlr4-python3-runtime, yacs, xmltodict, typing_extensions, terminaltables, tensorboard-data-server, shapely, regex, portalocker, opencv-python, omegaconf, munch, ftfy, cloudpickle, torch, iopath, hydra-core, typeguard, torchvision, pycocotools, motmetrics, jsonpickle, fvcore, timm, tensorboard, sacred, paddleseg, paddledet\r\n",
      "  Attempting uninstall: wcwidth\r\n",
      "    Found existing installation: wcwidth 0.1.7\r\n",
      "    Uninstalling wcwidth-0.1.7:\r\n",
      "      Successfully uninstalled wcwidth-0.1.7\r\n",
      "  Attempting uninstall: typing_extensions\r\n",
      "    Found existing installation: typing_extensions 4.3.0\r\n",
      "    Uninstalling typing_extensions-4.3.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.3.0\r\n",
      "  Attempting uninstall: opencv-python\r\n",
      "    Found existing installation: opencv-python 4.1.1.26\r\n",
      "    Uninstalling opencv-python-4.1.1.26:\r\n",
      "      Successfully uninstalled opencv-python-4.1.1.26\r\n",
      "  Attempting uninstall: cloudpickle\r\n",
      "    Found existing installation: cloudpickle 1.6.0\r\n",
      "    Uninstalling cloudpickle-1.6.0:\r\n",
      "      Successfully uninstalled cloudpickle-1.6.0\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.1.0\r\n",
      "    Uninstalling tensorboard-2.1.0:\r\n",
      "      Successfully uninstalled tensorboard-2.1.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "parl 1.4.1 requires cloudpickle==1.6.0; python_version >= \"3\", but you have cloudpickle 2.0.0 which is incompatible.\r\n",
      "parl 1.4.1 requires pyzmq==18.1.1, but you have pyzmq 23.2.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.8 cloudpickle-2.0.0 docopt-0.6.2 ftfy-6.1.1 fvcore-0.1.5.post20220119 hydra-core-1.1.1 iopath-0.1.10 jsonpickle-1.5.2 lap-0.4.0 motmetrics-1.4.0 munch-2.5.0 omegaconf-2.1.1 opencv-python-4.4.0.46 paddledet-2.6.0 paddleseg-2.7.0 portalocker-2.7.0 py-cpuinfo-9.0.0 pyclipper-1.3.0.post4 pycocotools-2.0.4 regex-2022.10.31 sacred-0.8.2 shapely-2.0.1 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 terminaltables-3.1.10 timm-0.4.12 torch-1.7.1 torchvision-0.8.2 typeguard-3.0.2 typing_extensions-4.5.0 wcwidth-0.2.6 xmltodict-0.13.0 yacs-0.1.8\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# 配置环境\n",
    "!cd /home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll && pwd && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-04-12T11:45:59.771750Z",
     "iopub.status.busy": "2023-04-12T11:45:59.771203Z",
     "iopub.status.idle": "2023-04-12T11:49:39.267763Z",
     "shell.execute_reply": "2023-04-12T11:49:39.266749Z",
     "shell.execute_reply.started": "2023-04-12T11:45:59.771705Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll\r\n",
      "scripts/infer.sh: 3: kill: Usage: kill [-s sigspec | -signum | -sigspec] [pid | job]... or\r\n",
      "kill -l [exitstatus]\r\n",
      "scripts/infer.sh: 5: kill: Usage: kill [-s sigspec | -signum | -sigspec] [pid | job]... or\r\n",
      "kill -l [exitstatus]\r\n",
      "W0412 19:46:05.639271  1284 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0412 19:46:05.642926  1284 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n",
      "0/17611\r\n",
      "1000/17611\r\n",
      "2000/17611\r\n",
      "3000/17611\r\n",
      "4000/17611\r\n",
      "5000/17611\r\n",
      "6000/17611\r\n",
      "7000/17611\r\n",
      "8000/17611\r\n",
      "9000/17611\r\n",
      "10000/17611\r\n",
      "11000/17611\r\n",
      "12000/17611\r\n",
      "13000/17611\r\n",
      "14000/17611\r\n",
      "15000/17611\r\n",
      "16000/17611\r\n",
      "17000/17611\r\n",
      "image_features.shape [17611, 512]\r\n",
      "text_features.shape [17611, 512]\r\n",
      "similarity_argsort.shape (17611, 17611)\r\n"
     ]
    }
   ],
   "source": [
    "# Track2 生成结果文件\n",
    "!cd /home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll && pwd && sh scripts/infer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T11:49:39.269634Z",
     "iopub.status.busy": "2023-04-12T11:49:39.269266Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll\r\n",
      "scripts/train.sh: 3: kill: Usage: kill [-s sigspec | -signum | -sigspec] [pid | job]... or\r\n",
      "kill -l [exitstatus]\r\n",
      "scripts/train.sh: 5: kill: Usage: kill [-s sigspec | -signum | -sigspec] [pid | job]... or\r\n",
      "kill -l [exitstatus]\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,083 -----------  Configuration  ----------------------\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,083 devices: None\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,083 elastic_level: -1\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,083 elastic_timeout: 30\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 gloo_port: 6767\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 host: None\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 job_id: default\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 legacy: False\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 log_dir: ./logs/vitbase_retrieval\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 log_level: INFO\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 master: None\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 max_restart: 3\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 nnodes: 1\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 nproc_per_node: None\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 rank: -1\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 run_mode: collective\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 server_num: None\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 servers: \r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 trainer_num: None\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 trainers: \r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 training_script: tools/ufo_train.py\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 training_script_args: ['--config-file', 'configs/vitbase_retrieval.py']\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 with_gloo: 0\r\n",
      "LAUNCH INFO 2023-04-12 19:49:44,084 --------------------------------------------------\r\n",
      "LAUNCH WARNING 2023-04-12 19:49:44,084 Compatible mode enable with args ['--gpus=0,1,2,3']\r\n",
      "-----------  Configuration Arguments -----------\r\n",
      "backend: auto\r\n",
      "cluster_topo_path: None\r\n",
      "elastic_pre_hook: None\r\n",
      "elastic_server: None\r\n",
      "enable_auto_mapping: False\r\n",
      "force: False\r\n",
      "gpus: 0,1,2,3\r\n",
      "heter_devices: \r\n",
      "heter_worker_num: None\r\n",
      "heter_workers: \r\n",
      "host: None\r\n",
      "http_port: None\r\n",
      "ips: 127.0.0.1\r\n",
      "job_id: None\r\n",
      "log_dir: ./logs/vitbase_retrieval\r\n",
      "np: None\r\n",
      "nproc_per_node: None\r\n",
      "rank_mapping_path: None\r\n",
      "run_mode: None\r\n",
      "scale: 0\r\n",
      "server_num: None\r\n",
      "servers: \r\n",
      "training_script: tools/ufo_train.py\r\n",
      "training_script_args: ['--config-file', 'configs/vitbase_retrieval.py']\r\n",
      "worker_num: None\r\n",
      "workers: \r\n",
      "------------------------------------------------\r\n",
      "WARNING 2023-04-12 19:49:44,085 launch.py:519] Not found distinct arguments and compiled with cuda or xpu or npu or mlu. Default use collective mode\r\n",
      "WARNING 2023-04-12 19:49:44,085 launch.py:519] Not found distinct arguments and compiled with cuda or xpu or npu or mlu. Default use collective mode\r\n",
      "launch train in GPU mode!\r\n",
      "INFO 2023-04-12 19:49:44,086 launch_utils.py:679] Change selected_gpus into reletive values. --ips:0,1,2,3 will change into relative_ips:[0, 1, 2, 3] according to your CUDA_VISIBLE_DEVICES:['0', '1', '2', '3']\r\n",
      "INFO 2023-04-12 19:49:44,086 launch_utils.py:679] Change selected_gpus into reletive values. --ips:0,1,2,3 will change into relative_ips:[0, 1, 2, 3] according to your CUDA_VISIBLE_DEVICES:['0', '1', '2', '3']\r\n",
      "INFO 2023-04-12 19:49:44,087 launch_utils.py:561] Local start 4 processes. First process distributed environment info (Only For Debug): \r\n",
      "    +=======================================================================================+\r\n",
      "    |                        Distributed Envs                      Value                    |\r\n",
      "    +---------------------------------------------------------------------------------------+\r\n",
      "    |                       PADDLE_TRAINER_ID                        0                      |\r\n",
      "    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:58657               |\r\n",
      "    |                     PADDLE_TRAINERS_NUM                        4                      |\r\n",
      "    |                PADDLE_TRAINER_ENDPOINTS  ... 0.1:54445,127.0.0.1:47641,127.0.0.1:34345|\r\n",
      "    |                     PADDLE_RANK_IN_NODE                        0                      |\r\n",
      "    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |\r\n",
      "    |                 PADDLE_WORLD_DEVICE_IDS                     0,1,2,3                   |\r\n",
      "    |                     FLAGS_selected_gpus                        0                      |\r\n",
      "    |             FLAGS_selected_accelerators                        0                      |\r\n",
      "    +=======================================================================================+\r\n",
      "\r\n",
      "INFO 2023-04-12 19:49:44,087 launch_utils.py:561] Local start 4 processes. First process distributed environment info (Only For Debug): \r\n",
      "    +=======================================================================================+\r\n",
      "    |                        Distributed Envs                      Value                    |\r\n",
      "    +---------------------------------------------------------------------------------------+\r\n",
      "    |                       PADDLE_TRAINER_ID                        0                      |\r\n",
      "    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:58657               |\r\n",
      "    |                     PADDLE_TRAINERS_NUM                        4                      |\r\n",
      "    |                PADDLE_TRAINER_ENDPOINTS  ... 0.1:54445,127.0.0.1:47641,127.0.0.1:34345|\r\n",
      "    |                     PADDLE_RANK_IN_NODE                        0                      |\r\n",
      "    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |\r\n",
      "    |                 PADDLE_WORLD_DEVICE_IDS                     0,1,2,3                   |\r\n",
      "    |                     FLAGS_selected_gpus                        0                      |\r\n",
      "    |             FLAGS_selected_accelerators                        0                      |\r\n",
      "    +=======================================================================================+\r\n",
      "\r\n",
      "INFO 2023-04-12 19:49:44,087 launch_utils.py:566] details about PADDLE_TRAINER_ENDPOINTS can be found in ./logs/vitbase_retrieval/endpoints.log, and detail running logs maybe found in ./logs/vitbase_retrieval/workerlog.0\r\n",
      "INFO 2023-04-12 19:49:44,087 launch_utils.py:566] details about PADDLE_TRAINER_ENDPOINTS can be found in ./logs/vitbase_retrieval/endpoints.log, and detail running logs maybe found in ./logs/vitbase_retrieval/workerlog.0\r\n",
      "launch proc_id:2083 idx:0\r\n",
      "launch proc_id:2088 idx:1\r\n",
      "launch proc_id:2093 idx:2\r\n",
      "launch proc_id:2098 idx:3\r\n",
      "rank is 0, world size is 4\r\n",
      "server not ready, wait 3 sec to retry...\r\n",
      "not ready endpoints:['127.0.0.1:54445', '127.0.0.1:47641', '127.0.0.1:34345']\r\n",
      "I0412 19:49:49.457259  2083 nccl_context.cc:83] init nccl context nranks: 4 local rank: 0 gpu id: 0 ring id: 0\r\n",
      "W0412 19:49:50.153883  2083 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W0412 19:49:50.156817  2083 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n",
      "./detectron2/config/lazy.py:154: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  return old_import(name, globals, locals, fromlist=fromlist, level=level)\r\n",
      "\u001b[32m[04/12 19:49:52 ufo]: \u001b[0mRank of current process: 0. World size: 1\r\n",
      "\u001b[32m[04/12 19:49:54 ufo]: \u001b[0mEnvironment info:\r\n",
      "----------------------  -------------------------------------------------------------------------------------\r\n",
      "sys.platform            linux\r\n",
      "Python                  3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]\r\n",
      "numpy                   1.19.5\r\n",
      "detectron2              imported a wrong installation\r\n",
      "detectron2._C           not built correctly: {e}\r\n",
      "Compiler ($CXX)         c++ (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\r\n",
      "CUDA compiler           Build cuda_11.2.r11.2/compiler.29618528_0\r\n",
      "DETECTRON2_ENV_MODULE   <not set>\r\n",
      "PyTorch                 1.7.1 @/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch\r\n",
      "PyTorch debug build     False\r\n",
      "GPU available           Yes\r\n",
      "GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch={cap})\r\n",
      "Driver version          460.32.03\r\n",
      "CUDA_HOME               /usr/local/cuda\r\n",
      "Pillow                  8.2.0\r\n",
      "torchvision             0.8.2 @/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torchvision\r\n",
      "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\r\n",
      "fvcore                  0.1.5.post20220119\r\n",
      "iopath                  0.1.10\r\n",
      "cv2                     4.4.0\r\n",
      "----------------------  -------------------------------------------------------------------------------------\r\n",
      "PyTorch built with:\r\n",
      "  - GCC 7.3\r\n",
      "  - C++ Version: 201402\r\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\r\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\r\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\r\n",
      "  - NNPACK is enabled\r\n",
      "  - CPU capability usage: AVX2\r\n",
      "  - CUDA Runtime 10.2\r\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75\r\n",
      "  - CuDNN 7.6.5\r\n",
      "  - Magma 2.5.2\r\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, U\r\n",
      "\r\n",
      "\u001b[32m[04/12 19:49:54 ufo]: \u001b[0mCommand line arguments: Namespace(config_file='configs/vitbase_retrieval.py', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)\r\n",
      "\u001b[32m[04/12 19:49:54 ufo]: \u001b[0mContents of args.config_file=configs/vitbase_retrieval.py:\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mcommon\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mtrain\u001b[39m\r\n",
      "\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mos\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15momegaconf\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mOmegaConf\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mcollections\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mOrderedDict\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdetectron2\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mconfig\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mLazyCall\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mas\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mL\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbuild\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mMultiTaskDataLoader\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmodeling\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmeta_arch\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mmultitask_v2\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mMultiTaskBatchFuse\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;242m# retrieval\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mtransforms\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbuild\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mbuild_transforms_lazy\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdata\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbuild_retrieval\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;15mbuild_retrieval_dataset\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m\\\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mbuild_retrieval_trainloader\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mbuild_retrieval_test_dataset\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15msolver\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbuild\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mbuild_lr_optimizer_lazy\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mbuild_lr_scheduler_lazy\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mOmegaConf\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcreate\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15m)\u001b[39m\r\n",
      "\u001b[38;5;15m_root\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mdatasets\u001b[39m\u001b[38;5;186m\"\u001b[39m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mMultiTaskDataLoader\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mcfg\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mdict\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15msample_mode\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mbatch\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mtask_loaders\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mOrderedDict\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15mretrieval\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mbuild_retrieval_trainloader\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m            \u001b[39m\u001b[38;5;15mdata_set\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mbuild_retrieval_dataset\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m                    \u001b[39m\u001b[38;5;15mdataset_name\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mRetrievalDataset\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m                    \u001b[39m\u001b[38;5;15mdataroot\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m_root\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m+\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m/train\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m                    \u001b[39m\u001b[38;5;15mtransforms\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mbuild_transforms_lazy\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m                        \u001b[39m\u001b[38;5;15mis_train\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m                        \u001b[39m\u001b[38;5;15msize_train\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m224\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m224\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m                        \u001b[39m\u001b[38;5;15mmean\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0.485\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.456\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.406\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m                        \u001b[39m\u001b[38;5;15mstd\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0.229\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.224\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.225\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m                    \u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m            \u001b[39m\u001b[38;5;15mtotal_batch_size\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m16\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\r\n",
      "\u001b[38;5;15m            \u001b[39m\u001b[38;5;15mworker_num\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\r\n",
      "\u001b[38;5;15m            \u001b[39m\u001b[38;5;15mdrop_last\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\r\n",
      "\u001b[38;5;15m            \u001b[39m\u001b[38;5;15mshuffle\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m            \u001b[39m\u001b[38;5;15mis_train\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m)\u001b[39m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmodeling\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mbackbones\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mvit_retrieval\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mCLIP\u001b[39m\r\n",
      "\u001b[38;5;197mfrom\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmodeling\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mheads\u001b[39m\u001b[38;5;15m.\u001b[39m\u001b[38;5;15mretrieval_head\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197mimport\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mRetrievalHead\u001b[39m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;5;15mbackbone\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mCLIP\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15membed_dim\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m512\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mimage_resolution\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m224\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mvision_layers\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m12\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mvision_width\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m768\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mvision_patch_size\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m32\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mcontext_length\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m77\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mvocab_size\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m49408\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mtransformer_width\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m512\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mtransformer_heads\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m8\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mtransformer_layers\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m12\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mqkv_bias\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mpre_norm\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mproj\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mpatch_bias\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mFalse\u001b[39m\r\n",
      "\u001b[38;5;15m)\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15mmodel\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mMultiTaskBatchFuse\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mbackbone\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mbackbone\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mheads\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mOrderedDict\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15mretrieval\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mRetrievalHead\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mpixel_mean\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0.485\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.456\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.406\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mpixel_std\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m0.229\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.224\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.225\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m)\u001b[39m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mbuild_lr_optimizer_lazy\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15moptimizer_type\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mAdamW\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mbase_lr\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m1e-4\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mweight_decay\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m1e-4\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mgrad_clip_enabled\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mTrue\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mgrad_clip_norm\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m0.1\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mapply_decay_param_fun\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;81mNone\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15mL\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mbuild_lr_scheduler_lazy\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m(\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15mmax_iters\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m900000\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15mwarmup_iters\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m0\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15msolver_steps\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;141m720000\u001b[39m\u001b[38;5;15m]\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15msolver_gamma\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m0.1\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15mbase_lr\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m1e-4\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m        \u001b[39m\u001b[38;5;15msched\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mPiecewiseDecay\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m,\u001b[39m\r\n",
      "\u001b[38;5;15m)\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mamp\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15menabled\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81mFalse\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;242m# data settings\u001b[39m\r\n",
      "\u001b[38;5;15msample_num\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m136117\u001b[39m\u001b[38;5;15m     \u001b[39m\u001b[38;5;242m#训练集样本量\u001b[39m\r\n",
      "\u001b[38;5;15mepochs\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m20\u001b[39m\r\n",
      "\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtask_loaders\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mretrieval\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtotal_batch_size\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\u001b[38;5;15m \u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15miters_per_epoch\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15msample_num\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m/\u001b[39m\u001b[38;5;197m/\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mdataloader\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtask_loaders\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mretrieval\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mtotal_batch_size\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15mmax_iters\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15miters_per_epoch\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mepochs\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;242m# optimizer\u001b[39m\r\n",
      "\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mmax_iters\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmax_iters\u001b[39m\r\n",
      "\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mbase_lr\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mlearning_rate\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1e-4\u001b[39m\r\n",
      "\u001b[38;5;15moptimizer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mlr_multiplier\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15msolver_steps\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15mint\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15mmax_iters\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\u001b[38;5;15m)\u001b[39m\u001b[38;5;15m]\u001b[39m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mmax_iter\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mmax_iters\u001b[39m\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcheckpointer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mperiod\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15mint\u001b[39m\u001b[38;5;15m(\u001b[39m\u001b[38;5;15miters_per_epoch\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\u001b[38;5;15m)\u001b[39m\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mcheckpointer\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mmax_to_keep\u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;141m10\u001b[39m\u001b[38;5;15m    \u001b[39m\u001b[38;5;242m# 只保存最新的10个模型\u001b[39m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15moutput_dir\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186moutputs/vitbase_retrieval\u001b[39m\u001b[38;5;186m'\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;242m# resume settings (remember last_checkpoint and --resume)\u001b[39m\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15mlog_period\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20\u001b[39m\r\n",
      "\r\n",
      "\u001b[38;5;15mtrain\u001b[39m\u001b[38;5;197m.\u001b[39m\u001b[38;5;15minit_checkpoint\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;197m=\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mpretrained/vitbase_clip.pdparams\u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;242m# 导入CLIP预训练模型\u001b[39m\r\n",
      "\r\n",
      "\u001b[32m[04/12 19:49:54 ufo]: \u001b[0mFull config saved to outputs/vitbase_retrieval/config.yaml\r\n",
      "mongo.txt does not exists, use file observer instead\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.context_length\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.embed_dim\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.image_resolution\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.patch_bias\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.pre_norm\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.proj\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.qkv_bias\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.transformer_heads\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.transformer_layers\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.transformer_width\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.vision_layers\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.vision_patch_size\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.vision_width\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.backbone.vocab_size\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.cfg.sample_mode\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.dataroot\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.dataset_name\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.transforms._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.transforms.is_train\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.transforms.mean\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.transforms.size_train\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.data_set.transforms.std\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.drop_last\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.is_train\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.shuffle\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.total_batch_size\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.dataloader.train.task_loaders.retrieval.worker_num\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.context_length\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.embed_dim\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.image_resolution\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.patch_bias\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.pre_norm\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.proj\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.qkv_bias\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.transformer_heads\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.transformer_layers\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.transformer_width\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.vision_layers\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.vision_patch_size\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.vision_width\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.backbone.vocab_size\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.heads._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.heads.retrieval._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.pixel_mean\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.model.pixel_std\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.apply_decay_param_fun\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.base_lr\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.grad_clip_enabled\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.grad_clip_norm\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier._target_\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.base_lr\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.learning_rate\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.max_iters\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.sched\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.solver_gamma\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.solver_steps\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.lr_multiplier.warmup_iters\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.optimizer_type\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.optimizer.weight_decay\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.amp.enabled\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.checkpointer.max_to_keep\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.checkpointer.period\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.cudnn_benchmark\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.ddp.broadcast_buffers\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.ddp.find_unused_parameters\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.ddp.fp16_compression\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.device\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.eval_period\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.init_checkpoint\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.log_period\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.max_iter\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.output_dir\"\r\n",
      "WARNING - root - Added new config entry: \"cfg_for_sacred.train.sacred.enabled\"\r\n",
      "INFO - outputs/vitbase_retrieval - Running command 'do_train_sacred'\r\n",
      "INFO - outputs/vitbase_retrieval - Started run with ID \"1\"\r\n",
      "data_set: <data.datasets.retrieval_dataset.RetrievalDataset object at 0x7f5c74840a10>\r\n",
      "RetrievalDataset has 126117 samples\r\n",
      "INFO - fastreid.data.samplers.data_sampler - dataset RetrievalDataset: rank 0 is mapped to _rank 0 under the real local world size 4\r\n",
      "\u001b[32m[04/12 19:49:57 ufo]: \u001b[0m{'train': {'output_dir': 'outputs/vitbase_retrieval', 'sacred': {'enabled': True}, 'init_checkpoint': 'pretrained/vitbase_clip.pdparams', 'amp': {'enabled': False}, 'cudnn_benchmark': True, 'ddp': {'broadcast_buffers': False, 'find_unused_parameters': True, 'fp16_compression': False}, 'max_iter': 5300, 'checkpointer': {'period': 2650, 'max_to_keep': 10}, 'eval_period': 5000, 'log_period': 20, 'device': 'gpu'}, 'dataloader': {'train': {'cfg': {'sample_mode': 'batch'}, 'task_loaders': {'retrieval': {'data_set': {'dataset_name': 'RetrievalDataset', 'dataroot': 'datasets/train', 'transforms': {'is_train': True, 'size_train': [224, 224], 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.120000000000005, 57.375], '_target_': <function build_transforms_lazy at 0x7f5c76ea89e0>}, '_target_': <function build_retrieval_dataset at 0x7f5c76eafb90>}, 'total_batch_size': 512, 'worker_num': 4, 'drop_last': True, 'shuffle': True, 'is_train': True, '_target_': <functio\r\n",
      "\u001b[32m[04/12 19:49:57 ufo]: \u001b[0mModel:\r\n",
      "MultiTaskBatchFuse(\r\n",
      "  (backbone): CLIP(\r\n",
      "    (visual): VisionTransformer(\r\n",
      "      (patch_embed): PatchEmbed(\r\n",
      "        (proj): Conv2D(3, 768, kernel_size=[32, 32], stride=[32, 32], data_format=NCHW)\r\n",
      "      )\r\n",
      "      (pos_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "      (norm_pre): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "      (blocks): LayerList(\r\n",
      "        (0): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (1): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (2): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (3): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (4): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (5): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (6): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (7): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (8): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (9): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (10): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (11): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=768, out_features=2304, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (norm_post): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "    )\r\n",
      "    (transformer): Transformer(\r\n",
      "      (blocks): LayerList(\r\n",
      "        (0): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (1): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (2): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (3): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (4): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (5): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (6): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (7): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (8): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (9): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (10): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (11): Block(\r\n",
      "          (norm1): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (attn): Attention(\r\n",
      "            (qkv): Linear(in_features=512, out_features=1536, dtype=float32)\r\n",
      "            (attn_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "            (proj): Linear(in_features=512, out_features=512, dtype=float32)\r\n",
      "            (proj_drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "          (drop_path): Identity()\r\n",
      "          (norm2): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "          (mlp): Mlp(\r\n",
      "            (fc1): Linear(in_features=512, out_features=2048, dtype=float32)\r\n",
      "            (act): QuickGELU()\r\n",
      "            (fc2): Linear(in_features=2048, out_features=512, dtype=float32)\r\n",
      "            (drop): Dropout(p=0.0, axis=None, mode=upscale_in_train)\r\n",
      "          )\r\n",
      "        )\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (token_embedding): Embedding(49408, 512, sparse=False)\r\n",
      "    (ln_final): LayerNorm(normalized_shape=[512], epsilon=1e-05)\r\n",
      "  )\r\n",
      "  (heads): LayerDict(\r\n",
      "    (retrieval): RetrievalHead(\r\n",
      "      (criterion): CrossEntropyLoss()\r\n",
      "    )\r\n",
      "  )\r\n",
      ")\r\n",
      "\u001b[32m[04/12 19:49:57 ufo]: \u001b[0mOptim:\r\n",
      "Weight Decay, params: \r\n",
      "INFO - fastreid.utils.checkpoint - Loading checkpoint from pretrained/vitbase_clip.pdparams\r\n",
      "WARNING - fastreid.utils.checkpoint - backbone.logit_scale has [1], but backbone.logit_scale has []\r\n",
      "WARNING - fastreid.utils.checkpoint - backbone.logit_scale has [], but backbone.logit_scale has [1]\r\n",
      "WARNING - fastreid.utils.checkpoint - missing keys: []\r\n",
      "WARNING - fastreid.utils.checkpoint - unexpected keys: []\r\n",
      "INFO - detectron2.engine.train_loop - Starting training from iteration 0\r\n",
      "INFO - detectron2.utils.events -  eta: 1:16:28  iter: 19  total_loss: 16.95  retrieval_loss_retrieval_img: 4.203  retrieval_loss_retrieval_text: 4.197  retrieval_loss_retrieval: 8.474  time: 0.8741  data_time: 0.0068  lr: 0.0001  \r\n",
      "INFO - detectron2.utils.events -  eta: 1:16:10  iter: 39  total_loss: 14.42  retrieval_loss_retrieval_img: 3.614  retrieval_loss_retrieval_text: 3.614  retrieval_loss_retrieval: 7.212  time: 0.8726  data_time: 0.0078  lr: 0.0001  \r\n",
      "INFO - detectron2.utils.events -  eta: 1:15:57  iter: 59  total_loss: 13.4  retrieval_loss_retrieval_img: 3.375  retrieval_loss_retrieval_text: 3.335  retrieval_loss_retrieval: 6.702  time: 0.8748  data_time: 0.0074  lr: 0.0001  \r\n",
      "INFO - detectron2.utils.events -  eta: 1:15:53  iter: 79  total_loss: 12.37  retrieval_loss_retrieval_img: 3.096  retrieval_loss_retrieval_text: 3.091  retrieval_loss_retrieval: 6.187  time: 0.8767  data_time: 0.0080  lr: 0.0001  \r\n",
      "INFO - detectron2.utils.events -  eta: 1:15:41  iter: 99  total_loss: 10.6  retrieval_loss_retrieval_img: 2.653  retrieval_loss_retrieval_text: 2.649  retrieval_loss_retrieval: 5.302  time: 0.8791  data_time: 0.0078  lr: 0.0001  \r\n"
     ]
    }
   ],
   "source": [
    "# Track2 模型训练\n",
    "!cd /home/aistudio/CVPR2023_foundation_model_Track2-main/OneForAll && pwd && sh scripts/train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
